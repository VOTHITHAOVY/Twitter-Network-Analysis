# analysis/centrality_analysis.py
import networkx as nx
import pandas as pd
import numpy as np
from collections import Counter

def centrality_analysis(G):
    """
    PhÃ¢n tÃ­ch centrality measures - ChÆ°Æ¡ng 3
    """
    print("\nğŸ¯ Báº®T Äáº¦U PHÃ‚N TÃCH CENTRALITY")
    print("=" * 50)
    
    # 1. TÃNH CÃC CENTRALITY MEASURES
    print("ğŸ“ˆ 1. TÃNH TOÃN CENTRALITY MEASURES...")
    
    centrality_results = {}
    
    # Degree Centrality
    print("   â€¢ Degree Centrality...")
    centrality_results['degree'] = nx.degree_centrality(G)
    
    # Betweenness Centrality (dÃ¹ng sampling cho máº¡ng lá»›n)
    print("   â€¢ Betweenness Centrality...")
    k = min(100, G.number_of_nodes())  # Sample size
    centrality_results['betweenness'] = nx.betweenness_centrality(G, k=k, seed=42)
    
    # Closeness Centrality
    print("   â€¢ Closeness Centrality...")
    centrality_results['closeness'] = nx.closeness_centrality(G)
    
    # PageRank
    print("   â€¢ PageRank...")
    centrality_results['pagerank'] = nx.pagerank(G, alpha=0.85)
    
    # Eigenvector Centrality (náº¿u cÃ³ thá»ƒ)
    try:
        print("   â€¢ Eigenvector Centrality...")
        centrality_results['eigenvector'] = nx.eigenvector_centrality(G, max_iter=1000)
    except:
        print("   â€¢ Eigenvector Centrality: KhÃ´ng thá»ƒ tÃ­nh (cÃ³ thá»ƒ do máº¡ng khÃ´ng liÃªn thÃ´ng)")
    
    print("âœ… ÄÃ£ tÃ­nh xong táº¥t cáº£ centrality measures")
    
    # 2. PHÃ‚N TÃCH TOP NODES
    print("\nğŸ† 2. TOP INFLUENCERS:")
    analyze_top_nodes(centrality_results, G)
    
    # 3. PHÃ‚N TÃCH TÆ¯Æ NG QUAN
    print("\nğŸ“Š 3. PHÃ‚N TÃCH TÆ¯Æ NG QUAN:")
    analyze_correlations(centrality_results)
    
    # 4. PHÃ‚N TÃCH THEO NHÃ“M
    print("\nğŸ“‹ 4. PHÃ‚N TÃCH THEO NHÃ“M:")
    analyze_node_categories(centrality_results, G)
    
    # 5. XUáº¤T Káº¾T QUáº¢
    print("\nğŸ’¾ 5. XUáº¤T Káº¾T QUáº¢:")
    export_centrality_results(centrality_results, G)
    
    print("\nâœ… HOÃ€N THÃ€NH PHÃ‚N TÃCH CENTRALITY")

def analyze_top_nodes(centrality_results, G, top_n=10):
    """PhÃ¢n tÃ­ch cÃ¡c nodes quan trá»ng nháº¥t"""
    
    print(f"\n   ğŸ“Š TOP {top_n} NODES THEO Tá»ªNG Äá»˜ ÄO:")
    
    top_nodes_by_measure = {}
    
    for measure_name, centrality_dict in centrality_results.items():
        if centrality_dict:  # Chá»‰ xá»­ lÃ½ náº¿u cÃ³ káº¿t quáº£
            sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]
            top_nodes_by_measure[measure_name] = [node for node, _ in sorted_nodes]
            
            print(f"\n   ğŸ”¹ {measure_name.upper()}:")
            for i, (node, score) in enumerate(sorted_nodes, 1):
                print(f"      {i:2d}. Node {node}: {score:.6f}")
    
    # TÃ¬m super influencers (xuáº¥t hiá»‡n trong nhiá»u top lists)
    print(f"\n   ğŸ¯ SUPER INFLUENCERS:")
    all_top_nodes = []
    for nodes in top_nodes_by_measure.values():
        all_top_nodes.extend(nodes)
    
    node_counts = Counter(all_top_nodes)
    super_influencers = [(node, count) for node, count in node_counts.items() if count >= 2]
    
    if super_influencers:
        super_influencers.sort(key=lambda x: x[1], reverse=True)
        for node, count in super_influencers:
            print(f"      â€¢ Node {node}: xuáº¥t hiá»‡n trong {count} top lists")
            
            # Hiá»ƒn thá»‹ scores cá»§a node nÃ y trong cÃ¡c measures
            scores_info = []
            for measure_name in centrality_results.keys():
                if measure_name in centrality_results and node in centrality_results[measure_name]:
                    score = centrality_results[measure_name][node]
                    scores_info.append(f"{measure_name}: {score:.4f}")
            print(f"        {', '.join(scores_info)}")
    else:
        print("      ğŸ¤” KhÃ´ng cÃ³ node nÃ o xuáº¥t hiá»‡n trong nhiá»u top lists")
    
    return top_nodes_by_measure, super_influencers

def analyze_correlations(centrality_results):
    """PhÃ¢n tÃ­ch tÆ°Æ¡ng quan giá»¯a cÃ¡c centrality measures"""
    
    # Táº¡o DataFrame cho táº¥t cáº£ nodes
    data = {}
    valid_measures = [name for name, results in centrality_results.items() if results]
    
    for measure_name in valid_measures:
        data[measure_name] = list(centrality_results[measure_name].values())
    
    # Chuyá»ƒn thÃ nh DataFrame
    min_length = min(len(values) for values in data.values())
    for key in data:
        data[key] = data[key][:min_length]
    
    df = pd.DataFrame(data)
    
    # TÃ­nh ma tráº­n tÆ°Æ¡ng quan
    correlation_matrix = df.corr()
    
    print("\n   ğŸ”— MA TRáº¬N TÆ¯Æ NG QUAN:")
    print("   " + " " * 12 + "".join([f"{col:12}" for col in correlation_matrix.columns]))
    for i, row_name in enumerate(correlation_matrix.index):
        row_str = f"   {row_name:12}"
        for j, col_name in enumerate(correlation_matrix.columns):
            row_str += f"{correlation_matrix.iloc[i, j]:12.3f}"
        print(row_str)
    
    # PhÃ¢n tÃ­ch cáº·p tÆ°Æ¡ng quan quan trá»ng
    print("\n   ğŸ“ˆ PHÃ‚N TÃCH TÆ¯Æ NG QUAN QUAN TRá»ŒNG:")
    for i in range(len(valid_measures)):
        for j in range(i + 1, len(valid_measures)):
            measure1 = valid_measures[i]
            measure2 = valid_measures[j]
            corr = correlation_matrix.loc[measure1, measure2]
            
            if abs(corr) > 0.7:
                strength = "Ráº¤T CAO" if abs(corr) > 0.8 else "CAO"
                direction = "dÆ°Æ¡ng" if corr > 0 else "Ã¢m"
                print(f"      â€¢ {measure1} vs {measure2}: {corr:.3f} ({strength}, {direction})")
    
    return correlation_matrix

def analyze_node_categories(centrality_results, G):
    """PhÃ¢n tÃ­ch nodes theo cÃ¡c nhÃ³m centrality"""
    
    print("\n   ğŸª PHÃ‚N LOáº I NODES THEO VAI TRÃ’:")
    
    # Láº¥y degree centrality lÃ m cÆ¡ sá»Ÿ
    if 'degree' in centrality_results:
        degree_centrality = centrality_results['degree']
        
        # PhÃ¢n loáº¡i theo degree
        degree_values = list(degree_centrality.values())
        thresholds = {
            'Ráº¥t tháº¥p': np.percentile(degree_values, 25),
            'Trung bÃ¬nh': np.percentile(degree_values, 50),
            'Cao': np.percentile(degree_values, 75),
            'Ráº¥t cao': np.percentile(degree_values, 90)
        }
        
        print("   ğŸ”¹ PHÃ‚N LOáº I THEO DEGREE CENTRALITY:")
        for category, threshold in thresholds.items():
            count = len([v for v in degree_values if v >= threshold])
            print(f"      â€¢ {category}: {count} nodes (â‰¥ {threshold:.4f})")
    
    # PhÃ¢n tÃ­ch nodes cÃ³ betweenness cao nhÆ°ng degree tháº¥p (cáº§u ná»‘i áº©n)
    if 'degree' in centrality_results and 'betweenness' in centrality_results:
        degree_threshold = np.percentile(list(degree_centrality.values()), 50)  # Median
        betweenness_threshold = np.percentile(list(centrality_results['betweenness'].values()), 75)  # Top 25%
        
        hidden_bridges = []
        for node in G.nodes():
            if (centrality_results['degree'][node] < degree_threshold and 
                centrality_results['betweenness'][node] > betweenness_threshold):
                hidden_bridges.append(node)
        
        print(f"\n   ğŸŒ‰ Cáº¦U Ná»I áº¨N ({len(hidden_bridges)} nodes):")
        if hidden_bridges:
            for node in hidden_bridges[:5]:  # Hiá»ƒn thá»‹ top 5
                print(f"      â€¢ Node {node}: degree={centrality_results['degree'][node]:.4f}, betweenness={centrality_results['betweenness'][node]:.4f}")
            if len(hidden_bridges) > 5:
                print(f"      â€¢ ... vÃ  {len(hidden_bridges) - 5} nodes khÃ¡c")
        else:
            print("      ğŸ¤” KhÃ´ng tÃ¬m tháº¥y cáº§u ná»‘i áº©n")

def export_centrality_results(centrality_results, G):
    """Xuáº¥t káº¿t quáº£ centrality ra file"""
    
    # Táº¡o DataFrame vá»›i táº¥t cáº£ centrality scores
    centrality_data = []
    for node in G.nodes():
        node_data = {'node_id': node}
        
        # ThÃªm degree thá»±c táº¿
        node_data['degree'] = G.degree(node)
        if G.is_directed():
            node_data['in_degree'] = G.in_degree(node)
            node_data['out_degree'] = G.out_degree(node)
        
        # ThÃªm centrality measures
        for measure_name, centrality_dict in centrality_results.items():
            if centrality_dict and node in centrality_dict:
                node_data[measure_name] = centrality_dict[node]
            else:
                node_data[measure_name] = None
        
        centrality_data.append(node_data)
    
    df_centrality = pd.DataFrame(centrality_data)
    df_centrality.to_csv('centrality_results.csv', index=False, encoding='utf-8')
    print("   ğŸ’¾ ÄÃ£ lÆ°u: centrality_results.csv")
    
    # Táº¡o file summary cho top nodes
    top_nodes_summary = []
    for measure_name, centrality_dict in centrality_results.items():
        if centrality_dict:
            sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)[:10]
            for rank, (node, score) in enumerate(sorted_nodes, 1):
                top_nodes_summary.append({
                    'measure': measure_name,
                    'rank': rank,
                    'node_id': node,
                    'score': score,
                    'degree': G.degree(node)
                })
    
    df_top = pd.DataFrame(top_nodes_summary)
    df_top.to_csv('top_centrality_nodes.csv', index=False, encoding='utf-8')
    print("   ğŸ’¾ ÄÃ£ lÆ°u: top_centrality_nodes.csv")
    
    # Thá»‘ng kÃª
    print(f"\n   ğŸ“Š THá»NG KÃŠ CENTRALITY:")
    print(f"      - Sá»‘ nodes Ä‘Æ°á»£c phÃ¢n tÃ­ch: {len(df_centrality)}")
    print(f"      - Sá»‘ centrality measures: {len(centrality_results)}")
    print(f"      - File Ä‘Ã£ xuáº¥t: centrality_results.csv, top_centrality_nodes.csv")

if __name__ == "__main__":
    # Test function
    print("ğŸ§ª TEST CENTRALITY ANALYSIS...")
    G = nx.erdos_renyi_graph(50, 0.2, seed=42)
    centrality_analysis(G)