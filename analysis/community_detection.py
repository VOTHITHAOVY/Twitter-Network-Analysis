# analysis/community_detection.py
import networkx as nx
import pandas as pd
import numpy as np
from community import community_louvain
from networkx.algorithms.community import greedy_modularity_communities, label_propagation_communities
import time
from collections import Counter

def community_analysis(G):
    """
    Ph√¢n t√≠ch community detection - Ch∆∞∆°ng 4
    """
    print("\nüë• B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH COMMUNITY DETECTION")
    print("=" * 50)
    
    # Chuy·ªÉn sang ƒë·ªì th·ªã v√¥ h∆∞·ªõng cho community detection
    G_undirected = G.to_undirected()
    
    # 1. SO S√ÅNH C√ÅC THU·∫¨T TO√ÅN
    print("üîç 1. SO S√ÅNH THU·∫¨T TO√ÅN COMMUNITY DETECTION:")
    algorithms_results = compare_community_algorithms(G_undirected)
    
    # 2. PH√ÇN T√çCH CHI TI·∫æT COMMUNITIES
    print("\nüìä 2. PH√ÇN T√çCH CHI TI·∫æT COMMUNITIES:")
    best_algorithm = select_best_algorithm(algorithms_results)
    detailed_analysis = analyze_communities_detail(G_undirected, algorithms_results[best_algorithm])
    
    # 3. K-CORE DECOMPOSITION
    print("\nüéØ 3. K-CORE DECOMPOSITION:")
    kcore_analysis = perform_kcore_analysis(G_undirected)
    
    # 4. K·∫æT H·ª¢P COMMUNITIES V√Ä CENTRALITY
    print("\nüîó 4. K·∫æT H·ª¢P COMMUNITIES V√Ä CENTRALITY:")
    analyze_communities_centrality(G, algorithms_results[best_algorithm])
    
    # 5. XU·∫§T K·∫æT QU·∫¢
    print("\nüíæ 5. XU·∫§T K·∫æT QU·∫¢:")
    export_community_results(algorithms_results, detailed_analysis, kcore_analysis, G)
    
    print("\n‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH COMMUNITY DETECTION")

def compare_community_algorithms(G):
    """So s√°nh c√°c thu·∫≠t to√°n community detection"""
    
    algorithms = {
        'Louvain': detect_louvain_communities,
        'Greedy Modularity': detect_greedy_modularity_communities,
        'Label Propagation': detect_label_propagation_communities
    }
    
    results = {}
    
    for algo_name, algo_func in algorithms.items():
        print(f"\n   üßÆ {algo_name}...")
        start_time = time.time()
        
        try:
            communities, modularity, additional_info = algo_func(G)
            execution_time = time.time() - start_time
            
            results[algo_name] = {
                'communities': communities,
                'modularity': modularity,
                'execution_time': execution_time,
                'n_communities': len(communities),
                'additional_info': additional_info
            }
            
            print(f"      ‚úÖ Th√†nh c√¥ng:")
            print(f"        - S·ªë communities: {len(communities)}")
            print(f"        - Modularity: {modularity:.4f}")
            print(f"        - Th·ªùi gian: {execution_time:.2f}s")
            
            if additional_info:
                for key, value in additional_info.items():
                    print(f"        - {key}: {value}")
                    
        except Exception as e:
            print(f"      ‚ùå Th·∫•t b·∫°i: {e}")
            results[algo_name] = None
    
    return results

def detect_louvain_communities(G):
    """Ph√°t hi·ªán communities b·∫±ng Louvain algorithm"""
    partition = community_louvain.best_partition(G)
    
    # Chuy·ªÉn partition th√†nh danh s√°ch communities
    communities_dict = {}
    for node, comm_id in partition.items():
        if comm_id not in communities_dict:
            communities_dict[comm_id] = []
        communities_dict[comm_id].append(node)
    
    communities = list(communities_dict.values())
    modularity = community_louvain.modularity(partition, G)
    
    # Th√¥ng tin th√™m
    additional_info = {
        'Community sizes': [len(comm) for comm in communities]
    }
    
    return communities, modularity, additional_info

def detect_greedy_modularity_communities(G):
    """Ph√°t hi·ªán communities b·∫±ng Greedy Modularity"""
    communities = list(greedy_modularity_communities(G))
    modularity = nx.algorithms.community.quality.modularity(G, communities)
    
    additional_info = {
        'Community sizes': [len(comm) for comm in communities]
    }
    
    return communities, modularity, additional_info

def detect_label_propagation_communities(G):
    """Ph√°t hi·ªán communities b·∫±ng Label Propagation"""
    communities = list(label_propagation_communities(G))
    
    # T√≠nh modularity cho label propagation
    partition = {}
    for i, comm in enumerate(communities):
        for node in comm:
            partition[node] = i
    modularity = community_louvain.modularity(partition, G)
    
    additional_info = {
        'Community sizes': [len(comm) for comm in communities]
    }
    
    return communities, modularity, additional_info

def select_best_algorithm(algorithms_results):
    """Ch·ªçn thu·∫≠t to√°n t·ªët nh·∫•t d·ª±a tr√™n modularity v√† th·ªùi gian"""
    
    valid_results = {name: result for name, result in algorithms_results.items() if result is not None}
    
    if not valid_results:
        print("   ‚ùå Kh√¥ng c√≥ thu·∫≠t to√°n n√†o ch·∫°y th√†nh c√¥ng")
        return None
    
    # ƒê√°nh gi√° d·ª±a tr√™n modularity (quan tr·ªçng h∆°n) v√† th·ªùi gian
    scores = {}
    for algo_name, result in valid_results.items():
        modularity_score = result['modularity']
        time_penalty = result['execution_time'] / 10  # Penalty cho th·ªùi gian d√†i
        total_score = modularity_score - time_penalty
        scores[algo_name] = total_score
    
    best_algorithm = max(scores.items(), key=lambda x: x[1])[0]
    best_result = valid_results[best_algorithm]
    
    print(f"\n   üèÜ THU·∫¨T TO√ÅN T·ªêT NH·∫§T: {best_algorithm}")
    print(f"      - Modularity: {best_result['modularity']:.4f}")
    print(f"      - Th·ªùi gian: {best_result['execution_time']:.2f}s")
    print(f"      - S·ªë communities: {best_result['n_communities']}")
    
    return best_algorithm

def analyze_communities_detail(G, algorithm_result):
    """Ph√¢n t√≠ch chi ti·∫øt communities"""
    
    communities = algorithm_result['communities']
    modularity = algorithm_result['modularity']
    
    print(f"\n   üìà PH√ÇN T√çCH CHI TI·∫æT COMMUNITIES:")
    print(f"      - T·ªïng s·ªë communities: {len(communities)}")
    print(f"      - Modularity: {modularity:.4f}")
    
    # Ph√¢n t√≠ch k√≠ch th∆∞·ªõc communities
    comm_sizes = [len(comm) for comm in communities]
    print(f"      - K√≠ch th∆∞·ªõc communities:")
    print(f"        ‚Ä¢ L·ªõn nh·∫•t: {max(comm_sizes)} nodes")
    print(f"        ‚Ä¢ Nh·ªè nh·∫•t: {min(comm_sizes)} nodes") 
    print(f"        ‚Ä¢ Trung b√¨nh: {np.mean(comm_sizes):.1f} nodes")
    print(f"        ‚Ä¢ ƒê·ªô l·ªách chu·∫©n: {np.std(comm_sizes):.1f}")
    
    # Ph√¢n lo·∫°i communities theo k√≠ch th∆∞·ªõc
    size_categories = {
        'R·∫•t nh·ªè (1-5 nodes)': len([s for s in comm_sizes if 1 <= s <= 5]),
        'Nh·ªè (6-15 nodes)': len([s for s in comm_sizes if 6 <= s <= 15]),
        'Trung b√¨nh (16-30 nodes)': len([s for s in comm_sizes if 16 <= s <= 30]),
        'L·ªõn (31-50 nodes)': len([s for s in comm_sizes if 31 <= s <= 50]),
        'R·∫•t l·ªõn (>50 nodes)': len([s for s in comm_sizes if s > 50])
    }
    
    print(f"      - Ph√¢n b·ªë k√≠ch th∆∞·ªõc:")
    for category, count in size_categories.items():
        if count > 0:
            percentage = (count / len(communities)) * 100
            print(f"        ‚Ä¢ {category}: {count} communities ({percentage:.1f}%)")
    
    # T√≠nh internal density cho m·ªói community
    print(f"\n      üéØ CH·∫§T L∆Ø·ª¢NG COMMUNITIES:")
    internal_densities = []
    
    for i, comm in enumerate(communities):
        if len(comm) > 1:
            subgraph = G.subgraph(comm)
            density = nx.density(subgraph)
            internal_densities.append(density)
        else:
            internal_densities.append(0)
    
    print(f"        ‚Ä¢ Internal density trung b√¨nh: {np.mean(internal_densities):.4f}")
    print(f"        ‚Ä¢ Internal density l·ªõn nh·∫•t: {max(internal_densities):.4f}")
    
    return {
        'n_communities': len(communities),
        'modularity': modularity,
        'community_sizes': comm_sizes,
        'internal_densities': internal_densities,
        'size_categories': size_categories
    }

def perform_kcore_analysis(G):
    """Th·ª±c hi·ªán K-core decomposition"""
    
    print(f"\n   üîç K-CORE DECOMPOSITION:")
    
    core_numbers = nx.core_number(G)
    max_k = max(core_numbers.values())
    
    print(f"      - Core number l·ªõn nh·∫•t: {max_k}")
    print(f"      - Core number trung b√¨nh: {np.mean(list(core_numbers.values())):.2f}")
    
    # Th·ªëng k√™ s·ªë nodes cho m·ªói k-core
    kcore_stats = {}
    for k in range(1, max_k + 1):
        k_core = nx.k_core(G, k)
        kcore_stats[k] = {
            'n_nodes': k_core.number_of_nodes(),
            'density': nx.density(k_core) if k_core.number_of_nodes() > 1 else 0
        }
    
    print(f"      - S·ªë nodes trong c√°c K-core:")
    for k in sorted(kcore_stats.keys()):
        stats = kcore_stats[k]
        percentage = (stats['n_nodes'] / G.number_of_nodes()) * 100
        print(f"        ‚Ä¢ {k}-core: {stats['n_nodes']} nodes ({percentage:.1f}%), density: {stats['density']:.4f}")
    
    return {
        'core_numbers': core_numbers,
        'max_k': max_k,
        'kcore_stats': kcore_stats
    }

def analyze_communities_centrality(G, algorithm_result):
    """Ph√¢n t√≠ch k·∫øt h·ª£p communities v√† centrality"""
    
    communities = algorithm_result['communities']
    
    # T√≠nh degree centrality cho to√†n m·∫°ng
    degree_centrality = nx.degree_centrality(G)
    
    print(f"\n   üåü INFLUENCERS TRONG COMMUNITIES:")
    
    for i, comm in enumerate(communities):
        if len(comm) >= 5:  # Ch·ªâ x√©t communities c√≥ √≠t nh·∫•t 5 nodes
            # T√¨m node c√≥ degree cao nh·∫•t trong community
            comm_degrees = [(node, degree_centrality[node]) for node in comm]
            top_node, top_degree = max(comm_degrees, key=lambda x: x[1])
            
            print(f"      ‚Ä¢ Community {i} ({len(comm)} nodes):")
            print(f"        - Influencer: Node {top_node} (degree centrality: {top_degree:.4f})")
            print(f"        - Top 3 nodes: {[node for node, _ in sorted(comm_degrees, key=lambda x: x[1], reverse=True)[:3]]}")

def export_community_results(algorithms_results, detailed_analysis, kcore_analysis, G):
    """Xu·∫•t k·∫øt qu·∫£ community analysis"""
    
    # Xu·∫•t k·∫øt qu·∫£ so s√°nh thu·∫≠t to√°n
    algo_data = []
    for algo_name, result in algorithms_results.items():
        if result:
            algo_data.append({
                'algorithm': algo_name,
                'n_communities': result['n_communities'],
                'modularity': result['modularity'],
                'execution_time': result['execution_time'],
                'status': 'Success'
            })
        else:
            algo_data.append({
                'algorithm': algo_name,
                'n_communities': 0,
                'modularity': 0,
                'execution_time': 0,
                'status': 'Failed'
            })
    
    df_algorithms = pd.DataFrame(algo_data)
    df_algorithms.to_csv('community_algorithms_comparison.csv', index=False, encoding='utf-8')
    print("   üíæ ƒê√£ l∆∞u: community_algorithms_comparison.csv")
    
    # Xu·∫•t k·∫øt qu·∫£ K-core
    kcore_data = []
    for k, stats in kcore_analysis['kcore_stats'].items():
        kcore_data.append({
            'k_value': k,
            'n_nodes': stats['n_nodes'],
            'density': stats['density'],
            'percentage': (stats['n_nodes'] / G.number_of_nodes()) * 100
        })
    
    df_kcore = pd.DataFrame(kcore_data)
    df_kcore.to_csv('kcore_analysis.csv', index=False, encoding='utf-8')
    print("   üíæ ƒê√£ l∆∞u: kcore_analysis.csv")
    
    # Xu·∫•t community assignments (cho thu·∫≠t to√°n t·ªët nh·∫•t)
    best_algorithm = select_best_algorithm(algorithms_results)
    if best_algorithm:
        communities = algorithms_results[best_algorithm]['communities']
        community_assignments = []
        
        for comm_id, comm_nodes in enumerate(communities):
            for node in comm_nodes:
                community_assignments.append({
                    'node_id': node,
                    'community_id': comm_id,
                    'community_size': len(comm_nodes)
                })
        
        df_assignments = pd.DataFrame(community_assignments)
        df_assignments.to_csv('community_assignments.csv', index=False, encoding='utf-8')
        print("   üíæ ƒê√£ l∆∞u: community_assignments.csv")
    
    print(f"\n   üìä TH·ªêNG K√ä COMMUNITY ANALYSIS:")
    print(f"      - S·ªë thu·∫≠t to√°n so s√°nh: {len(algorithms_results)}")
    print(f"      - S·ªë communities (best): {detailed_analysis['n_communities']}")
    print(f"      - Modularity (best): {detailed_analysis['modularity']:.4f}")
    print(f"      - K-core max: {kcore_analysis['max_k']}")

if __name__ == "__main__":
    # Test function
    print("üß™ TEST COMMUNITY DETECTION...")
    G = nx.erdos_renyi_graph(100, 0.1, seed=42)
    community_analysis(G)